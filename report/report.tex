\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath,amssymb,amsthm}

\linespread{1.1}

\newtheorem{lemma}{Lemma}
\newtheorem*{lem}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}
\newtheorem*{claim}{Claim}
\newtheorem*{fclaim}{False Claim}
\newtheorem{observation}{Observation}
\newtheorem{conjecture}[lemma]{Conjecture}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem*{rt}{Running Time}

\setlength{\textwidth}{6.3in}
\renewcommand{\labelenumi}{\bf \alph{enumi}.}
\renewcommand{\labelitemi}{--}

\renewcommand{\maketitle}{
  \begin{center}
    \begin{flushright}
      Sayer Rippey, Oren Shoham, Peter Fogg \\
      CS364 \\
      Final Report
    \end{flushright}
    \rule{\linewidth}{0.1mm}
  \end{center}
}

\begin{document}
\maketitle
\subsection*{Introduction}
How do you know that you'll like a beer before you even try it? Machine learning! The goal of our project was to create a machine learning-based recommender system along the lines of Pandora or Netflix, but for beers. In other words, we wanted to make a program that would take in an input beer and return a list of similar beers that the user might like. A beer, like a piece of music or a movie, requires a complex description that incorporates a number of different attributes. In order to measure similarity between beers, we first needed to find a way to quantify these attributes. We obviously didn't have time to create a quantitative description of every beer in the world by hand, \`{a} la Pandora. However, we did have one thing --- the Internet! Specifically, beer review websites. We scraped a bunch of reviews from a popular site and wrote a relatively simple NLP progam to generate attribute vectors for each beer.  \\
\indent There are two main approaches to designing a recommender system: content-based filtering and collaborative filtering. Content-based filtering algorithms compare items based on the characteristics of those items. Collaborative filtering algorithms, on the other hand, compare items based on the users who like or dislike them (e.g. you like $x$, other people who like $x$ also like $y$, so you might like $y$). We tried both approaches - one that recommends beers that have similar language in their reviews, and one that recommends beers liked by users who also like the input beer.

\subsection*{Problem Definition and Algorithms}
Formally, our problem can be formulated as: \\
\indent {\bf Input:} a beer name and a number $k$, supplied by the user \\
\indent {\bf Goal:} a list of $k$ similar beers that the user might also like \\
\\

Our content-based algorithm takes in an input beer and returns the $k$ beers whose attribute vectors are closest to that of the input beer in terms of Manhattan distance. Our collaborative algorithm takes in an input beer, compiles a list of beers that users who liked the input beer also liked (we assumed that a user "liked"  a beer if they gave it a score of 4.0 out of 5.0 or higher), and returns the $k$ most highly rated beers on that list.

%% Our hybrid algorithm takes in an input beer, compiles a list of beers that users who liked the input beer also liked, and returns the $k$ beers from that list whose attribute vectors are most similar to that of the input beer in terms of Manhattan distance.


\subsection*{System Design}
The system is broken up into three parts -- data collection, data processing, and the core recommendation engine.
\begin{itemize}
\item \textbf{Collection.} 904,149 reviews were scraped from BeerAdvocate\footnote{http://beeradvocate.com}, using the BeautifulSoup library to parse HTML. The text of each individual review was saved to a SQLite database, along with the score, author's username, and the associated beer's name, brewery of origin, and percent ABV.

\item \textbf{Processing.} After obtaining the data, we examined the 400 most commonly used words across all reviews. From these, we hand-selected a set of 85 particularly descriptive words. Our hypothesis was that words like ``grapefruit'', ``hoppy'', or ``toasted'' would appear frequently in reviews of beers with those attributes, and that similar beers would have similar frequencies of attribute usage in their reviews. For each attribute, we summed the number of appearances of the attribute in all reviews of a particular beer and normalized by the number of reviews for that beer. Each of the 9442 beers is represented as a vector of these attributes, with each attribute value in the range $[0, 1]$. Since we were originally focused on a content-based approach, these are geared toward being useful for knn.py, and the collaborative approach, collab.py, is slower as a result.
\item \textbf{Recommendation.} For the content-based approach, we use a $k$-nearest neighbors-based algorithm to determine similar beers. Given an input beer and a $k$, the beer's vector representation is retrieved from the database, and all other beers are compared against it using a simple Manhattan distance function. The nearest $k$ beers are then printed out, sorted in increasing order of distance from the input. 

As an additional feature, to give more guidance as to which recommended beers the user will enjoy, we print out the most similar attributes (i.e. those with the most similar non-zero values) between the input beer and a recommendation -- for example, searching for Sierra Nevada Torpedo tells me that Hop Notch is similar in the attributes ``bite'', ``apple'', and ``foamy''. We also tell the user the most important attributes of a particular beer, which are assumed to be those which appear most commonly in reviews (i.e. the attributes with the highest values). Torpedo's most important attributes are ``hop'', ``pine'', and ``red''.

For the collaborative approach, we get the usernames of every review author who rated the input beer highly, and find all the other beers they enjoyed. We then give the user the $k$ highest-rated beers in that list, and the usernames of some of the authors who liked it. This, while arguably creepy, could allow a user to look at the reviews of those authors, and decide they don't agree with the program, or, if they do agree, they could find new beer suggestions or make a friend.
  
  Finally, beer names are somewhat inconsistently formatted in our database, and the user might not remember the precise name of the beer they want. For example, the user might search for ``Torpedo'', but the database contains ``Sierra Nevada Torpedo Extra IPA''. In order to compensate for this, we implemented a fuzzy string matching heuristic to suggest similarly-named beers. If the input is a substring of a beer name, this distance heuristic is 0. Otherwise, it is the length of the longest common subsequence plus 3 over the Levenshtein distance of the two strings. We return a sorted list of beer names with a sufficiently small distance, and present these to the user as options. The heuristic is somewhat arbitrary, but it usually suggests the desired beer as the first result.

\end{itemize}
\subsection*{Evaluation}
Given that beer preferences are an entirely subjective matter, we used a qualitative approach to our testing. When prompted with a beer that the user likes, the system should respond with similar beers. Better yet, the results should contain other beers that the user likes -- this is a good indication that the system is capable of determining the user's preferences.

Our results are promising. The content-based system always returns beers of a similar style and quality, and often gives beers that the testers are known to enjoy. The collaborative-based system, while returning a wider spectrum of beer types, also seems to return similar quality beers. Hands-on testing indicates that beers unknown to the tester are highly likely to be enjoyed.

What's particularly interesting about the content-based system is that the results are almost always of the same type -- for example, given a stout as input, the suggestions are mostly stouts and porters. The system knows nothing about different styles of brewing, but nonetheless is able to essentially identify the genre.

Currently, performance is significantly different between the two algorithms. The content-based system is quite fast; all it needs to do is compute a simple function for around 9500 beers. The collaborative system, though, is quite slow at times. Because it needs to examine all 900,000 reviews in order to find users with similar preferences, it can be extremely slow. It also performs worse on more popular beers, because it then needs to examine more users who liked the beer. Much of the sluggishness of the algorithm is due to database access; by switching to a more scalable non-relational database such as Redis or MongoDB we might alleviate this problem. The algorithm may still require reworking, due to the size of the dataset.

\subsection*{Future Work}
There are a few directions that could be taken going forward. Currently the system only uses one beer as input. While this is effective, it is somewhat limiting -- an IPA as an input beer generally gives all IPAs or pale ales as recommendations, because the system has no knowledge of the user's diversity of taste. By giving a varied input set, the system can give a less restricted set of recommendations. This would require a different algorithmic approach; support-vector machines would likely work well here.

We can also track the user's preferences over time, by creating an account for each user. This would allow the user to receive a few suggestions, head to the store, try them out, and get updated recommendations based on the new data. This allows the possibility of an upvote-downvote system, similar to Pandora's. The system currently uses only two classifications, ``liked'' or ``unknown''. Adding downvotes allows an explicit ``disliked'' classification.

The idea of a user account immediately suggests deploying this as a webapp. While not theoretically interesting, this would bring the benefits of beer recommendations to the entire world.

\subsection*{Conclusion}
Using simple NLP techniques, we were able to concisely describe complex attributes of a beer's flavor. After that, the actual recommendation is a simple distance function and a few sorts. Despite the apparent lack of sophistication, our system is quite effective. It successfully identifies the most important attributes of a beer, and is able to accurately suggest other beers which the user likes.
\end{document}
